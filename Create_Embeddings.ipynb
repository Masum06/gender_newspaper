{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create_Embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Masum06/gender_newspaper/blob/master/Create_Embeddings.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "m_25g4bA04EO",
        "colab_type": "code",
        "colab": {},
        "outputId": "104a19b7-8992-430b-c13a-b3cf988711b0"
      },
      "cell_type": "code",
      "source": [
        "import gensim, logging, os, re, string, tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "print('gensim version: \\t%s'     % gensim.__version__)\n",
        "print('TensorFlow version: \\t%s' % tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gensim version: \t3.4.0\n",
            "TensorFlow version: \t1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sdyO_P8Y04Ec",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Config"
      ]
    },
    {
      "metadata": {
        "id": "WEzbU6TM04Ed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For displaying gensim logs\n",
        "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Directory with raw txt-files\n",
        "TEXT_DIR  = 'data/yelp/train'\n",
        "\n",
        "# Directory for saving checkpoint and metadata\n",
        "MODEL_DIR = 'emb_yelp/'\n",
        "\n",
        "# Word2vec\n",
        "EMBEDDING_SIZE = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6oT2adf04Ej",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "uBYwRLzg04El",
        "colab_type": "code",
        "colab": {},
        "outputId": "f6028a7d-ba90-4fc2-f094-a7525a00a236"
      },
      "cell_type": "code",
      "source": [
        "def clean_doc(doc):\n",
        "    \"\"\"\n",
        "    Cleaning a document by several methods\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    doc = doc.lower()\n",
        "    # Remove numbers\n",
        "    doc = re.sub(r\"[0-9]+\", \"\", doc)\n",
        "    # Split in tokens\n",
        "    tokens = doc.split()\n",
        "    # Remove punctuation\n",
        "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
        "    # Tokens with less then two characters will be ignored\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def read_files(path):\n",
        "    \"\"\"\n",
        "    Read in text files\n",
        "    \"\"\"\n",
        "    documents = list()\n",
        "    tokenize  = lambda x: simple_preprocess(x)\n",
        "    \n",
        "    # Read in all files in directory\n",
        "    if os.path.isdir(path):\n",
        "        for filename in os.listdir(path):\n",
        "            with open('%s/%s' % (path, filename), encoding='utf-8') as f:\n",
        "                doc = f.read()\n",
        "                doc = clean_doc(doc)\n",
        "                documents.append(tokenize(doc))\n",
        "    return documents\n",
        "\n",
        "docs = read_files(TEXT_DIR)\n",
        "print('Number of documents: %i' % len(docs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wsq8hSKY04Er",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training model"
      ]
    },
    {
      "metadata": {
        "id": "QoX4Ntsa04Es",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5e390b4-a38e-4c4f-e2a3-54c7fbc2f651"
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(docs, size=EMBEDDING_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : collecting all words and their counts\n",
            "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO : PROGRESS: at sentence #10000, processed 1332433 words, keeping 34615 word types\n",
            "INFO : PROGRESS: at sentence #20000, processed 2682863 words, keeping 51033 word types\n",
            "INFO : PROGRESS: at sentence #30000, processed 4013459 words, keeping 63788 word types\n",
            "INFO : PROGRESS: at sentence #40000, processed 5386954 words, keeping 75163 word types\n",
            "INFO : PROGRESS: at sentence #50000, processed 6725433 words, keeping 85157 word types\n",
            "INFO : PROGRESS: at sentence #60000, processed 8069244 words, keeping 94511 word types\n",
            "INFO : PROGRESS: at sentence #70000, processed 9429856 words, keeping 103236 word types\n",
            "INFO : PROGRESS: at sentence #80000, processed 10766708 words, keeping 111540 word types\n",
            "INFO : PROGRESS: at sentence #90000, processed 12117230 words, keeping 119401 word types\n",
            "INFO : PROGRESS: at sentence #100000, processed 13487814 words, keeping 127178 word types\n",
            "INFO : PROGRESS: at sentence #110000, processed 14465090 words, keeping 134272 word types\n",
            "INFO : PROGRESS: at sentence #120000, processed 15432222 words, keeping 141021 word types\n",
            "INFO : PROGRESS: at sentence #130000, processed 16408624 words, keeping 147231 word types\n",
            "INFO : PROGRESS: at sentence #140000, processed 17386678 words, keeping 153494 word types\n",
            "INFO : PROGRESS: at sentence #150000, processed 18368502 words, keeping 159326 word types\n",
            "INFO : PROGRESS: at sentence #160000, processed 19358391 words, keeping 165233 word types\n",
            "INFO : PROGRESS: at sentence #170000, processed 20342281 words, keeping 170920 word types\n",
            "INFO : PROGRESS: at sentence #180000, processed 21323355 words, keeping 176227 word types\n",
            "INFO : PROGRESS: at sentence #190000, processed 22297528 words, keeping 181834 word types\n",
            "INFO : collected 186895 word types from a corpus of 23280688 raw words and 200000 sentences\n",
            "INFO : Loading a fresh vocabulary\n",
            "INFO : min_count=5 retains 42113 unique words (22% of original 186895, drops 144782)\n",
            "INFO : min_count=5 leaves 23078591 word corpus (99% of original 23280688, drops 202097)\n",
            "INFO : deleting the raw counts dictionary of 186895 items\n",
            "INFO : sample=0.001 downsamples 58 most-common words\n",
            "INFO : downsampling leaves estimated 17763378 word corpus (77.0% of prior 23078591)\n",
            "INFO : estimated required memory for 42113 words and 300 dimensions: 122127700 bytes\n",
            "INFO : resetting layer weights\n",
            "INFO : training model with 3 workers on 42113 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "INFO : PROGRESS: at 0.52% examples, 527873 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 1.07% examples, 539312 words/s, in_qsize 6, out_qsize 0\n",
            "INFO : PROGRESS: at 1.60% examples, 541588 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 2.15% examples, 545192 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 2.69% examples, 545475 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 3.22% examples, 543218 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 3.76% examples, 545142 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 4.31% examples, 546955 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 4.84% examples, 546151 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 5.39% examples, 546073 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 5.92% examples, 545658 words/s, in_qsize 4, out_qsize 1\n",
            "INFO : PROGRESS: at 6.46% examples, 545864 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 6.98% examples, 545495 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 7.52% examples, 545294 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 8.07% examples, 545783 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 8.62% examples, 546252 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 9.16% examples, 546600 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 9.69% examples, 546823 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 10.30% examples, 546768 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 11.04% examples, 546609 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 11.80% examples, 546783 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 12.52% examples, 546174 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 13.24% examples, 546075 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 14.00% examples, 546585 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 14.73% examples, 546676 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 15.47% examples, 546709 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 16.18% examples, 546396 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 16.94% examples, 546453 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 17.67% examples, 546600 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 18.39% examples, 546197 words/s, in_qsize 4, out_qsize 1\n",
            "INFO : PROGRESS: at 19.14% examples, 546115 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 19.88% examples, 546115 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 20.46% examples, 546388 words/s, in_qsize 4, out_qsize 1\n",
            "INFO : PROGRESS: at 21.01% examples, 546688 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 21.55% examples, 546876 words/s, in_qsize 6, out_qsize 0\n",
            "INFO : PROGRESS: at 22.11% examples, 547233 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 22.66% examples, 547603 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 23.20% examples, 547571 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 23.74% examples, 547800 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 24.28% examples, 547952 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 24.83% examples, 548022 words/s, in_qsize 3, out_qsize 2\n",
            "INFO : PROGRESS: at 25.36% examples, 547882 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 25.89% examples, 548033 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 26.44% examples, 547905 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 26.98% examples, 548200 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 27.52% examples, 548113 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 28.07% examples, 548427 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 28.61% examples, 548230 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 29.14% examples, 547988 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 29.66% examples, 548090 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 30.27% examples, 548131 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 31.04% examples, 548392 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 31.79% examples, 548429 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 32.52% examples, 548368 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 33.24% examples, 548270 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 33.95% examples, 547915 words/s, in_qsize 3, out_qsize 2\n",
            "INFO : PROGRESS: at 34.70% examples, 547991 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 35.43% examples, 547805 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 36.13% examples, 547498 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 36.88% examples, 547647 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 37.63% examples, 547632 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 38.36% examples, 547557 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 39.12% examples, 547681 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 39.86% examples, 547717 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 40.45% examples, 547849 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 41.01% examples, 548044 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 41.55% examples, 548228 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 42.10% examples, 548395 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 42.66% examples, 548437 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 43.20% examples, 548535 words/s, in_qsize 3, out_qsize 2\n",
            "INFO : PROGRESS: at 43.75% examples, 548726 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 44.29% examples, 548860 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 44.84% examples, 548925 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 45.39% examples, 549001 words/s, in_qsize 5, out_qsize 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO : PROGRESS: at 45.92% examples, 549012 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 46.49% examples, 549227 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 47.02% examples, 549361 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 47.57% examples, 549318 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 48.12% examples, 549365 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 48.66% examples, 549447 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 49.21% examples, 549622 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 49.75% examples, 549740 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 50.39% examples, 549725 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 51.15% examples, 549719 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 51.92% examples, 549813 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 52.65% examples, 549782 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 53.41% examples, 549901 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 54.15% examples, 549811 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 54.90% examples, 549840 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 55.62% examples, 549644 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 56.36% examples, 549618 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 57.09% examples, 549449 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 57.81% examples, 549398 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 58.55% examples, 549401 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 59.28% examples, 549371 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 60.03% examples, 549347 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 60.57% examples, 549318 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 61.12% examples, 549323 words/s, in_qsize 4, out_qsize 1\n",
            "INFO : PROGRESS: at 61.64% examples, 549296 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 62.19% examples, 549347 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 62.75% examples, 549331 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 63.29% examples, 549420 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 63.81% examples, 549315 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 64.35% examples, 549314 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 64.89% examples, 549266 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 65.44% examples, 549330 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 65.97% examples, 549304 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 66.51% examples, 549280 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 67.03% examples, 549260 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 67.60% examples, 549395 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 68.15% examples, 549503 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 68.69% examples, 549521 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 69.23% examples, 549548 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 69.76% examples, 549564 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 70.39% examples, 549458 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 71.12% examples, 549349 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 71.88% examples, 549374 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 72.61% examples, 549396 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 73.35% examples, 549361 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 74.10% examples, 549307 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 74.83% examples, 549175 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 75.57% examples, 549201 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 76.31% examples, 549271 words/s, in_qsize 4, out_qsize 1\n",
            "INFO : PROGRESS: at 77.05% examples, 549209 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 77.79% examples, 549232 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 78.56% examples, 549267 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 79.29% examples, 549316 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 80.03% examples, 549249 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 80.59% examples, 549350 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 81.15% examples, 549341 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 81.69% examples, 549397 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 82.24% examples, 549470 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 82.79% examples, 549511 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 83.33% examples, 549472 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 83.87% examples, 549573 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 84.41% examples, 549599 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 84.96% examples, 549604 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 85.50% examples, 549616 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 86.05% examples, 549657 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 86.58% examples, 549599 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 87.10% examples, 549618 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 87.67% examples, 549664 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 88.21% examples, 549730 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 88.74% examples, 549650 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 89.29% examples, 549723 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 89.83% examples, 549779 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 90.52% examples, 549807 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 91.27% examples, 549750 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 92.01% examples, 549717 words/s, in_qsize 6, out_qsize 1\n",
            "INFO : PROGRESS: at 92.76% examples, 549762 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 93.49% examples, 549764 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 94.24% examples, 549758 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 94.99% examples, 549757 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 95.72% examples, 549731 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 96.46% examples, 549755 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 97.20% examples, 549792 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 97.94% examples, 549756 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 98.68% examples, 549703 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : PROGRESS: at 99.42% examples, 549712 words/s, in_qsize 5, out_qsize 0\n",
            "INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "INFO : training on 116403440 raw words (88814925 effective words) took 161.6s, 549745 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "SbK2C9X904Ex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Saving model"
      ]
    },
    {
      "metadata": {
        "id": "u4_HFeHS04Ey",
        "colab_type": "code",
        "colab": {},
        "outputId": "dea7b51c-6542-4937-8885-4eaff7599634"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "model.save(os.path.join(MODEL_DIR,'word2vec'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : saving Word2Vec object under emb_yelp/word2vec, separately None\n",
            "INFO : not storing attribute vectors_norm\n",
            "INFO : not storing attribute cum_table\n",
            "INFO : saved emb_yelp/word2vec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "n4p3tjbN04E2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating checkpoint and metadata"
      ]
    },
    {
      "metadata": {
        "id": "44zXB4xv04E3",
        "colab_type": "code",
        "colab": {},
        "outputId": "b6fbffb7-531a-4917-b0b0-3bda596ca2aa"
      },
      "cell_type": "code",
      "source": [
        "weights     = model.wv.vectors\n",
        "index_words = model.wv.index2word\n",
        "\n",
        "vocab_size    = weights.shape[0]\n",
        "embedding_dim = weights.shape[1]\n",
        "\n",
        "print('Shape of weights:', weights.shape)\n",
        "print('Vocabulary size: %i' % vocab_size)\n",
        "print('Embedding size: %i'  % embedding_dim)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR,'metadata.tsv'), 'w') as f:\n",
        "    f.writelines(\"\\n\".join(index_words))\n",
        "\n",
        "# Required if you re-run without restarting the kernel\n",
        "tf.reset_default_graph()\n",
        "\n",
        "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
        "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
        "\n",
        "embedding_init = W.assign(embedding_placeholder)\n",
        "writer = tf.summary.FileWriter(MODEL_DIR, graph=tf.get_default_graph())\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = W.name\n",
        "embedding.metadata_path = './metadata.tsv'\n",
        "projector.visualize_embeddings(writer, config)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(embedding_init, feed_dict={embedding_placeholder: weights})\n",
        "    save_path = saver.save(sess, os.path.join(MODEL_DIR, \"model.cpkt\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of weights: (42113, 300)\n",
            "Vocabulary size: 42113\n",
            "Embedding size: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8F5mr25D04E7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example"
      ]
    },
    {
      "metadata": {
        "id": "4eUrm3IL04E-",
        "colab_type": "code",
        "colab": {},
        "outputId": "a3198be8-612a-48b3-af2a-71582ea74c1f"
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['coffee'], topn=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('espresso', 0.6709840893745422),\n",
              " ('latte', 0.6611574292182922),\n",
              " ('cappuccino', 0.6460868716239929),\n",
              " ('tea', 0.643097996711731),\n",
              " ('lattes', 0.613446056842804),\n",
              " ('coffees', 0.612466037273407),\n",
              " ('teas', 0.5807890295982361),\n",
              " ('chai', 0.567467451095581),\n",
              " ('mocha', 0.565311074256897),\n",
              " ('gelato', 0.5606527328491211)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}